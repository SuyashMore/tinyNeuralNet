{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tinyNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnOq0knDWAHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irjP05cynx7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltRWOjYudFBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Activation Functions\n",
        "\n",
        "\n",
        "#'der' refers to Derivative of the corresponding Function - X is the Cached Value of Activation Function\n",
        "def activation_sigmoid(X,der=False):\n",
        "    if not der:\n",
        "        return np.divide(1, 1 + np.exp(-X) )\n",
        "    else:\n",
        "        return np.multiply(X,(1-X))\n",
        "\n",
        "def activation_linear(X,der=False):\n",
        "    if not der:\n",
        "        return X\n",
        "    else:\n",
        "        return np.ones(X.shape)\n",
        "\n",
        "\n",
        "def activation_relu(X,der=False):\n",
        "    if not der:\n",
        "        return np.multiply((X>=0),X)\n",
        "    else:\n",
        "        return (X>=0).astype(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxZP8q_51H9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cost Functions\n",
        "\n",
        "def crossEntropyCost(Y,Y_orig,der=False):\n",
        "    if not der:\n",
        "        m=Y.shape[1]\n",
        "        cost = -np.sum(Y_orig*np.log(Y)+(1-Y_orig)*np.log(1-Y))/m\n",
        "        cost = np.squeeze(cost)\n",
        "        assert(cost.shape == ())\n",
        "        return cost\n",
        "    else:\n",
        "        dLda = -(np.divide(Y_orig, Y) - np.divide(1 - Y_orig, 1 - Y))  \n",
        "        return dLda\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N6Sp0m3YCNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork():\n",
        "    def __init__(self):\n",
        "        self.layerSizes = []\n",
        "        self.activations = []\n",
        "\n",
        "    def debugPrint(self,x):\n",
        "        if self.debug:\n",
        "            print(x)\n",
        "\n",
        "    def compile(self,lr=0.9,debug=False):\n",
        "\n",
        "        self.debug=debug\n",
        "        self.lr = lr\n",
        "\n",
        "        # Initialize Empty Arrays to Store Weights and Biases\n",
        "        self.totalLayers = len(self.layerSizes)-1\n",
        "        self.weights = [None]*self.totalLayers\n",
        "        self.bias = [None]*self.totalLayers\n",
        "\n",
        "        # Initialize Weight and Bias Matrix \n",
        "        for i in range (len(self.layerSizes)-1):\n",
        "\n",
        "            # Dimensions of Weight Matrix : (layer i+1 , layer i),Bias:(layer i,1)\n",
        "\n",
        "            self.weights[i]=np.random.randn(self.layerSizes[i+1],self.layerSizes[i])*0.01\n",
        "            self.bias[i]=np.random.randn(self.layerSizes[i+1],1)*0.01\n",
        "            \n",
        "            assert(self.weights[i].shape==(self.layerSizes[i+1],self.layerSizes[i]))\n",
        "            assert(self.bias[i].shape==(self.layerSizes[i+1],1))\n",
        "\n",
        "        print(\"Model Compiled Successfully\")\n",
        "\n",
        "    def addLayer(self,layerSize,activation=None):\n",
        "        self.layerSizes.append(layerSize)\n",
        "\n",
        "        if len(self.layerSizes)==1: # No Activation on Input Layer\n",
        "            assert(activation==None) \n",
        "        else:\n",
        "            assert(activation!=None) # Activation Required\n",
        "            self.activations.append(activation)\n",
        "\n",
        "    \n",
        "    def forward(self,X):\n",
        "        Z = None\n",
        "        A = X\n",
        "        self.activationCache = [] # Used to Compute Gradients\n",
        "\n",
        "        self.debugPrint(\"=================Model Meta=====================\")\n",
        "        self.debugPrint(f\"Layers(Including Input) :{self.layerSizes}\")\n",
        "        self.debugPrint(f\"Activations:{self.activations}\")\n",
        "        self.activationCache.append(X)\n",
        "        # Forward Propogate through Each Layer\n",
        "        for i in range(self.totalLayers):\n",
        "            Z = np.dot(self.weights[i],A)+self.bias[i] # Z= W.X + b\n",
        "            A = self.activations[i](Z)                  # A = activation(Z)\n",
        "            self.activationCache.append(A)\n",
        "\n",
        "            self.debugPrint(\"======================================================\")\n",
        "            self.debugPrint(f\"Layer #{i+1} Output:\\n{A}\")\n",
        "        self.debugPrint(\"================== End Of Model =================================\")\n",
        "        self.last_Output = A\n",
        "        return A\n",
        "\n",
        "\n",
        "    def fit(self,X,Y,epochs=1):\n",
        "        # Feed Forward\n",
        "        result = self.forward(X)\n",
        "        \n",
        "        #Compute Cost\n",
        "        cost = crossEntropyCost(result,Y)\n",
        "        print(f\"Initial Cost:{cost}\")\n",
        "\n",
        "        for i in range(epochs):\n",
        "            # Feed Forward\n",
        "            result = self.forward(X)\n",
        "\n",
        "            #Compute Gradients\n",
        "            dWs,dbs = self.compute_grads(Y)\n",
        "\n",
        "            #Apply Gradients\n",
        "            self.apply_grads(dWs,dbs)\n",
        "\n",
        "            # Compute Cost\n",
        "            result = self.forward(X)\n",
        "            cost = crossEntropyCost(result,Y)\n",
        "            print(f\"Epoch ({i}/{epochs-1})=> Cost:{cost}\")\n",
        "\n",
        "\n",
        "\n",
        "    def apply_grads(self,dWs,dbs):\n",
        "        for i in range(self.totalLayers):\n",
        "            self.weights[i] -= self.lr * dWs[i]\n",
        "            self.bias[i] -= self.lr * dbs[i]\n",
        "\n",
        "\n",
        "\n",
        "    def compute_grads(self,Y): # Returns Cost and Grads(dws,dbs)\n",
        "\n",
        "        def linear_backward(dZ,cached_activation,weights,bias): #Calculate grads for Layer i\n",
        "            #cached_activation(activation in layer i),weights(weight Matrix of ith layer),bias(bias matrix of ith layer)\n",
        "            m=cached_activation.shape[1] #For Vetorized Implementation\n",
        "            # print(f\"dZ = {dZ}\")\n",
        "            # print(f\"Cached Act:{cached_activation}\")\n",
        "            dW = np.dot(dZ,cached_activation.T)/m\n",
        "            db = np.sum(dZ,axis=1,keepdims=True)\n",
        "            dA = np.dot(weights.T,dZ)\n",
        "            # print(f\"dW = {dW}\")\n",
        "\n",
        "            assert (dW.shape == weights.shape)\n",
        "            assert (db.shape == bias.shape)\n",
        "            assert (dA.shape == cached_activation.shape)\n",
        "            \n",
        "\n",
        "            return (dW,db,dA)\n",
        "\n",
        "        \n",
        "        dLda = crossEntropyCost(self.last_Output,Y,der=True)\n",
        "\n",
        "\n",
        "        dZs = [None]*self.totalLayers\n",
        "        dWs = [None]*self.totalLayers\n",
        "        dbs = [None]*self.totalLayers\n",
        "        \n",
        "\n",
        "        dZs[-1]=dLda\n",
        "        # print(f\"dZs:{dZs}\")\n",
        "        for l in reversed(range(self.totalLayers)):\n",
        "            # print(f\"Layer #{l} : {self.layerSizes[l]}\")\n",
        "            dadz = self.activations[l](self.activationCache[l+1],der=True)\n",
        "            layer_cache = self.activationCache[l]\n",
        "            dldz = np.multiply(dZs[l],dadz)\n",
        "            dw,db,dA = linear_backward(dldz,layer_cache,self.weights[l],self.bias[l])\n",
        "            dZs[l-1]=dA\n",
        "            dWs[l]=dw\n",
        "            dbs[l]=db\n",
        "\n",
        "        return dWs,dbs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "nn = NeuralNetwork()\n",
        "nn.addLayer(2)\n",
        "nn.addLayer(6,activation_sigmoid)\n",
        "nn.addLayer(1,activation_sigmoid)\n",
        "nn.compile(lr=2.15)\n",
        "\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "Y = np.array([[1],[0],[0],[1]])\n",
        "print(f\"Input:{X}\")\n",
        "# res = nn.forward(X)\n",
        "# print(f\"result:{res}\")\n",
        "\n",
        "print(f\"Target:{Y}\")\n",
        "\n",
        "nn.fit(X.T,Y.T,2000)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEh-gr4aYUGM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a06e7937-049a-45ae-af09-146b739a17eb"
      },
      "source": [
        ""
      ],
      "execution_count": 341,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00022479]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 341
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGc6PWWxahuH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "46891ba6-7ab5-42b3-c853-e516044dcbed"
      },
      "source": [
        "for i in reversed(range(5)):\n",
        "    print(i)"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y66HjM2ogPAO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bfd10aa3-de70-4fd2-d077-fa89e925f32c"
      },
      "source": [
        "a = np.array([1,2,3,4,5,6,7])\n",
        "(a>3).astype(float)\n"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuiA-6BMmh2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}